{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ec4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66987fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.tree import DecisionTreeClassifier   # for Task 4\n",
    "from sklearn.base import clone                    # optional for Task 4\n",
    "import matplotlib.pyplot as plt                   # for Task 5\n",
    "from sklearn.metrics.pairwise import rbf_kernel   # for Task 6e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0474faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"Base class for a contextual multi-armed bandit (MAB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    # initialise and raise input errors\n",
    "    def __init__(self, n_arms):\n",
    "        if not type(n_arms)==int:\n",
    "            raise TypeError(\"`n_arms` must be an integer\")\n",
    "        if not n_arms >= 0:\n",
    "            raise ValueError(\"`n_arms` must be non-negative\")\n",
    "        self.n_arms = n_arms\n",
    "        \n",
    "    @abstractmethod\n",
    "    # raise input errors\n",
    "    def play(self, context):\n",
    "        \"\"\"Play a round\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors presented to the MAB. The 0-th \n",
    "            axis indexes the arms, and the 1-st axis indexes the features.\n",
    "            Non-contextual bandits accept a context of None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arm : int\n",
    "            Integer index of the arm played this round. Should be in the set \n",
    "            {0, ..., n_arms - 1}.\n",
    "        \"\"\"\n",
    "        if not type(context) == np.ndarray:\n",
    "            raise TypeError(\"`context` must be numpy.ndarray\")\n",
    "        if not context.shape == (n_arms, n_dims):\n",
    "            raise TypeError(\"`context` must have shape (n_arms, n_dims)\")\n",
    "        self.context = context\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    # raise input errors\n",
    "    def update(self, arm, reward, context):\n",
    "        \"\"\"Update the internal state of the MAB after a play\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
    "        \n",
    "        reward : float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors that was presented to the MAB. The \n",
    "            0-th axis indexes the arms, and the 1-st axis indexes the \n",
    "            features. Non-contextual bandits accept a context of None. \n",
    "        \"\"\"\n",
    "        if not (type(arm) == int or arm.dtype == 'int64'):\n",
    "            raise TypeError(\"`arm` must be int type\")\n",
    "        if not (arm >= 0 and arm <= (n_arms-1)):\n",
    "            raise ValueError(\"`arm` must be the the set {0, .., n_arms - 1}\")\n",
    "        if not (type(reward) == float or reward.dtype == 'float64'):\n",
    "            raise TypeError(\"`reward` must be float type\")\n",
    "        if not (context.shape == (n_arms, n_dims) and context.dtype == 'float64') :\n",
    "            raise TypeError(\"`context` must be float numpy in shape (n_events, n_arms, n_dims)\")\n",
    "        # get the values\n",
    "        self.arm = arm\n",
    "        self.reward = reward\n",
    "        self.context = context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbff5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global functions \n",
    "def break_tie(_range):\n",
    "    indices = np.argwhere(_range == np.max(_range))\n",
    "    index = np.random.randint(0,len(indices))\n",
    "\n",
    "    return indices[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ce893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset here\n",
    "\n",
    "data = np.loadtxt(\"dataset.txt\")\n",
    "arms, rewards, contexts = data[:,0], data[:,1], data[:,2:]\n",
    "arms = arms.astype(int)\n",
    "rewards = rewards.astype(float)\n",
    "contexts = contexts.astype(float)\n",
    "n_arms = len(np.unique(arms))\n",
    "n_events = len(contexts)\n",
    "n_dims = int(len(contexts[0])/n_arms)\n",
    "contexts = contexts.reshape(n_events, n_arms, n_dims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd31754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offlineEvaluate(mab, arms, rewards, contexts, n_rounds=None):\n",
    "    \"\"\"Offline evaluation of a multi-armed bandit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab : instance of MAB\n",
    "        MAB to evaluate.\n",
    "    \n",
    "    arms : integer numpy.ndarray, shape (n_events,) \n",
    "        Array containing the history of pulled arms, represented as integer \n",
    "        indices in the set {0, ..., mab.n_arms}\n",
    "    \n",
    "    rewards : float numpy.ndarray, shape (n_events,)\n",
    "        Array containing the history of rewards.\n",
    "    \n",
    "    contexts : float numpy.ndarray, shape (n_events, n_arms, n_dims)\n",
    "        Array containing the history of contexts presented to the arms. \n",
    "        The 0-th axis indexes the events in the history, the 1-st axis \n",
    "        indexes the arms and the 2-nd axis indexed the features.\n",
    "        \n",
    "    n_rounds : int, default=None\n",
    "        Number of matching events to evaluate the MAB on. If None, \n",
    "        continue evaluating until the historical events are exhausted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : float numpy.ndarray\n",
    "        Rewards for the matching events.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    if not (arms.shape == (n_events,) and arms.dtype == 'int64')  :\n",
    "        raise TypeError(\"`arms` must be integer numpy in shape (n_events,)\")\n",
    "    if not rewards.shape == (n_events,) and rewards.dtype == 'float64' :\n",
    "        raise TypeError(\"`rewards` must be float numpy in shape (n_events,)\")\n",
    "    if not contexts.shape == (n_events,n_arms, n_dims) and rewards.dtype == 'float64' :\n",
    "        raise TypeError(\"`contexts` must be float numpy in shape (n_events, n_arms, n_dims)\")\n",
    "    if n_rounds == None:        # set n_rounds to infinite number to run until all data exhausted\n",
    "        n_rounds = np.inf\n",
    "    elif not type(n_rounds) == int:\n",
    "        raise TypeError(\"`n_rounds` must be integer or default 'None'\")\n",
    "\n",
    "    n_round = 0     # count the current round ; 0 indicates the first round\n",
    "    R = []          # save the total payoff\n",
    "    H = []          # save used historical events\n",
    "    \n",
    "    for i in range(n_events):\n",
    "        if n_round == n_rounds:\n",
    "            break\n",
    "        arm = mab.play(contexts[i])\n",
    "        if arm == arms[i]:                 # if historical data equals to chosen arm\n",
    "            R.append(rewards[i])           # append the new rewards\n",
    "            H.append([arms[i], rewards[i], contexts[i]])      # append the used events\n",
    "            mab.update(arms[i], rewards[i], contexts[i])      # update the information\n",
    "            n_round += 1\n",
    "\n",
    "    # return rewards per play\n",
    "    out = np.array(R)\n",
    "        \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b9ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy(MAB):\n",
    "    \"\"\"Epsilon-Greedy multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms\n",
    "\n",
    "    epsilon : float\n",
    "        Explore probability. Must be in the interval [0, 1].\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    def __init__(self, n_arms, epsilon, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        if not (epsilon >= 0 and epsilon <= 1):\n",
    "            raise ValueError(\"`epsilon` must be a number in [0,1]\")\n",
    "        if not type(epsilon) == float:\n",
    "            raise TypeError(\"`epsilon` must be float\")\n",
    "        if not type(Q0) == float:\n",
    "            raise TypeError(\"`Q0` must be a float number or default value 'np.inf'\")\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.q = np.full(n_arms, Q0)      # initialise q values\n",
    "        self.rewards = np.zeros(n_arms)     # keep the total rewards per arm\n",
    "        self.clicks = np.zeros(n_arms)      # count the pulled rounds per arm\n",
    "    \n",
    "    # select a random arm to explore or a arm with best rewards to exploit, then return the arm \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        if np.random.random_sample() <= self.epsilon:           #explore\n",
    "            arm = np.random.randint(0,self.n_arms) \n",
    "        else:\n",
    "            arm = break_tie(self.q)\n",
    "        return arm\n",
    "    \n",
    "    # update values\n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.clicks[arm] += 1\n",
    "        self.rewards[arm] += self.reward\n",
    "        self.q[arm] = self.rewards[arm] / self.clicks[arm]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee8e3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpsGreedy average reward 0.18125\n"
     ]
    }
   ],
   "source": [
    "mab = EpsGreedy(10, 0.05) \n",
    "results_EpsGreedy = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('EpsGreedy average reward', np.mean(results_EpsGreedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d64cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(MAB):\n",
    "    \"\"\"Upper Confidence Bound (UCB) multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    rho : float\n",
    "        Positive real explore-exploit parameter.\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, rho, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        if not rho > 0:\n",
    "            raise ValueError(\"`rho` must be positive\")\n",
    "        if not (type(rho) == float and np.isreal(rho)):\n",
    "            raise TypeError(\"`rho` must be real float\")\n",
    "        if not type(Q0) == float :\n",
    "            raise TypeError(\"`Q0` must be a float number or default value 'np.inf'\")\n",
    "            \n",
    "        self.rho = rho\n",
    "        self.q = np.full(n_arms, Q0)\n",
    "        self.rewards = np.zeros(n_arms)  \n",
    "        self.avg_rewards = np.zeros(n_arms)\n",
    "        self.clicks = np.zeros(n_arms)\n",
    "        self.round = 0        # to count the number of round played\n",
    "    \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        self.round += 1\n",
    "        self.q = np.where(self.clicks != 0, self.avg_rewards + np.sqrt(self.rho * np.log10(self.round) / self.clicks), self.q)\n",
    "\n",
    "        arm = break_tie(self.q)\n",
    "        \n",
    "        return int(arm)\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.clicks[arm] += 1\n",
    "        self.rewards[arm] += reward\n",
    "        self.avg_rewards[arm] = self.rewards[arm]/ self.clicks[arm]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c43624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB average reward 0.18472652218782248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/l_xlgyyj413fr5n1ym7q20rc0000gn/T/ipykernel_1898/87646695.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  self.q = np.where(self.clicks != 0, self.avg_rewards + np.sqrt(self.rho * np.log10(self.round) / self.clicks), self.q)\n",
      "/var/folders/9k/l_xlgyyj413fr5n1ym7q20rc0000gn/T/ipykernel_1898/87646695.py:34: RuntimeWarning: divide by zero encountered in divide\n",
      "  self.q = np.where(self.clicks != 0, self.avg_rewards + np.sqrt(self.rho * np.log10(self.round) / self.clicks), self.q)\n"
     ]
    }
   ],
   "source": [
    "mab = UCB(10, 1.0)\n",
    "results_UCB = offlineEvaluate(mab, arms, rewards, contexts,)\n",
    "print('UCB average reward', np.mean(results_UCB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(MAB):\n",
    "    \"\"\"Contextual multi-armed bandit (LinUCB)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    alpha : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    def __init__(self, n_arms, n_dims, alpha):\n",
    "        if not (type(n_dims) == int or n_dims.dtype == 'int64'):\n",
    "            raise TypeError(\"`n_dims` must be integer type\")\n",
    "        if not (type(alpha) == float or alpha.dtype == 'float64'):\n",
    "            raise TypeError(\"`alpha` must be float\")\n",
    "        if not (alpha > 0.0 and np.isreal(alpha)):\n",
    "            raise ValueError(\"`alpha` must be positive real\")\n",
    "        \n",
    "        super().__init__(n_arms) \n",
    "        self.n_dims = n_dims\n",
    "        self.alpha = alpha\n",
    "        self.post_dist = np.zeros(n_dims)\n",
    "        '''initialise keys and values; key is arm, A for covariance, inv_A for inverse of A, \n",
    "                                        b for reward, theta for coefficient vector''' \n",
    "        self.A = np.array(np.identity(n_dims))\n",
    "        self.inv_A = [np.linalg.inv(self.A)]*10\n",
    "        self.A  = [self.A]*10\n",
    "\n",
    "        self.b = [np.zeros(n_dims)]*10\n",
    "        self.theta = [(np.linalg.inv(np.identity(n_dims)) @  np.zeros(n_dims))]*10\n",
    "         \n",
    "    # return the best arm\n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # calculate posterior distribution of the coefficient vector \n",
    "        for arm in range(self.n_arms):\n",
    "            inv_A = self.inv_A[arm]\n",
    "            theta = self.theta[arm]\n",
    "\n",
    "            # calculate posterior distribution of the coefficient vector\n",
    "            self.post_dist[arm] = theta @ context[arm] + self.alpha * np.sqrt(context[arm].T @ inv_A @ context[arm])\n",
    "            \n",
    "        arm = break_tie(self.post_dist)\n",
    "        return int(arm)    \n",
    "    \n",
    "    # update dictionary\n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        reshaped_context = context[arm].reshape(-1,1)   # reshape to the right form\n",
    "        self.A[arm] = self.A[arm] + reshaped_context @ reshaped_context.T\n",
    "        self.inv_A[arm] = np.linalg.inv(self.A[arm])\n",
    "        self.b[arm] = self.b[arm] + reward * context[arm]\n",
    "        self.theta[arm] = self.inv_A[arm] @ self.b[arm]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50cfe38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinUCB average reward 0.53\n"
     ]
    }
   ],
   "source": [
    "mab = LinUCB(10, 10, 1.0)\n",
    "results_LinUCB = offlineEvaluate(mab, arms, rewards, contexts,800)\n",
    "print('LinUCB average reward', np.mean(results_LinUCB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06e30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
